{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmSnQdERflnB",
        "outputId": "a7721d47-4009-4757-fd23-470d94892f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DCFSL-2021-main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My Drive/DCFSL-2021-main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python DCFSL-UP.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02s-BJGQg_17",
        "outputId": "cfaf4ddf-9ff1-4ae6-a74e-79c539313120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'Labels', 'set'])\n",
            "[1 1 1 ... 5 1 1]\n",
            "(42776, 9, 9, 103)\n",
            "(42776,)\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}\n",
            "dict_keys([1, 2, 5, 0, 7, 3, 6, 8, 4])\n",
            "Num classes for source domain datasets: 9\n",
            "dict_keys([1, 2, 5, 0, 7, 3, 6, 8, 4])\n",
            "the number of class: 9\n",
            "the number of sample: 1800\n",
            "Num classes of the number of class larger than 200: 9\n",
            "9 dict_keys([1, 2, 5, 0, 7, 3, 6, 8, 4])\n",
            "(42776, 9, 9, 103)\n",
            "(9, 9, 103, 42776)\n",
            "[1 1 1 ... 5 1 1]\n",
            "paviaU 610 340 103\n",
            "(610, 340, 103)\n",
            "number of sample 42776\n",
            "labeled number per class: 5\n",
            "40.0\n",
            "40\n",
            "the number of train_indices: 45\n",
            "the number of test_indices: 42731\n",
            "the number of train_indices after data argumentation: 1800\n",
            "labeled sample indices: [36730, 1284, 8004, 22951, 6553, 5366, 27740, 33403, 29082, 5317, 14521, 19329, 17679, 26047, 22829, 1037, 2221, 581, 37194, 23536, 8213, 6063, 7117, 8053, 9746, 13743, 14303, 18044, 13268, 14290, 16245, 16885, 14714, 15679, 21438, 26367, 25409, 7081, 6505, 26010, 24165, 24368, 26351, 11276, 19897]\n",
            "Data is OK.\n",
            "ok\n",
            "train labels: tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
            "        4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8])\n",
            "size of train datas: torch.Size([45, 103, 9, 9])\n",
            "dict_keys(['data', 'Labels', 'set'])\n",
            "(9, 9, 103, 1800)\n",
            "[0 0 0 ... 8 8 8]\n",
            "(1800, 103, 9, 9)\n",
            "target data augmentation label: [0 0 0 ... 8 8 8]\n",
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
            "(9, 9, 103, 1800)\n",
            "[0 0 0 ... 8 8 8]\n",
            "Training...\n",
            "Testing ...\n",
            "2.331305980682373\n",
            "-2.95308518409729\n",
            "\t\tAccuracy: 23898/42731 (55.93%)\n",
            "\n",
            "save networks for episode: 1\n",
            "/content/drive/MyDrive/DCFSL-2021-main/DCFSL-UP.py:630: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  A[iDataSet, :] = np.diag(C) / np.sum(C, 1, dtype=np.float)\n",
            "best episode:[1], best accuracy=55.92661065736819\n",
            "episode 100:  domain loss: 0.7418, fsl loss: 0.0610, acc 0.7407, loss: 0.8028\n",
            "episode 200:  domain loss: 0.5964, fsl loss: 0.0322, acc 0.8019, loss: 0.6287\n",
            "episode 300:  domain loss: 0.6997, fsl loss: 0.0113, acc 0.8327, loss: 0.7110\n",
            "episode 400:  domain loss: 0.7146, fsl loss: 0.0726, acc 0.8546, loss: 0.7873\n",
            "episode 500:  domain loss: 0.7345, fsl loss: 0.0024, acc 0.8716, loss: 0.7369\n",
            "episode 600:  domain loss: 0.5471, fsl loss: 0.0064, acc 0.8845, loss: 0.5535\n",
            "episode 700:  domain loss: 0.6502, fsl loss: 0.0442, acc 0.8940, loss: 0.6944\n",
            "episode 800:  domain loss: 0.7661, fsl loss: 0.0051, acc 0.9005, loss: 0.7712\n",
            "episode 900:  domain loss: 0.6988, fsl loss: 0.0179, acc 0.9060, loss: 0.7167\n",
            "episode 1000:  domain loss: 0.6902, fsl loss: 0.0012, acc 0.9116, loss: 0.6914\n",
            "Testing ...\n",
            "3.9273152351379395\n",
            "-4.682089805603027\n",
            "\t\tAccuracy: 33383/42731 (78.12%)\n",
            "\n",
            "save networks for episode: 1000\n",
            "/content/drive/MyDrive/DCFSL-2021-main/DCFSL-UP.py:630: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  A[iDataSet, :] = np.diag(C) / np.sum(C, 1, dtype=np.float)\n",
            "best episode:[1000], best accuracy=78.12361049355269\n",
            "episode 1100:  domain loss: 0.7293, fsl loss: 0.0003, acc 0.9175, loss: 0.7296\n",
            "episode 1200:  domain loss: 0.6891, fsl loss: 0.0018, acc 0.9229, loss: 0.6909\n",
            "episode 1300:  domain loss: 0.7030, fsl loss: 0.0017, acc 0.9269, loss: 0.7047\n",
            "episode 1400:  domain loss: 0.7571, fsl loss: 0.0011, acc 0.9310, loss: 0.7582\n",
            "episode 1500:  domain loss: 0.6930, fsl loss: 0.0002, acc 0.9349, loss: 0.6932\n",
            "episode 1600:  domain loss: 0.6972, fsl loss: 0.0022, acc 0.9382, loss: 0.6994\n",
            "episode 1700:  domain loss: 0.7355, fsl loss: 0.0022, acc 0.9410, loss: 0.7376\n",
            "episode 1800:  domain loss: 0.6776, fsl loss: 0.0039, acc 0.9428, loss: 0.6815\n",
            "episode 1900:  domain loss: 0.6985, fsl loss: 0.0007, acc 0.9449, loss: 0.6991\n",
            "episode 2000:  domain loss: 0.7665, fsl loss: 0.0004, acc 0.9468, loss: 0.7669\n",
            "Testing ...\n",
            "6.110958099365234\n",
            "-6.962102890014648\n",
            "\t\tAccuracy: 33445/42731 (78.27%)\n",
            "\n",
            "save networks for episode: 2000\n",
            "/content/drive/MyDrive/DCFSL-2021-main/DCFSL-UP.py:630: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  A[iDataSet, :] = np.diag(C) / np.sum(C, 1, dtype=np.float)\n",
            "best episode:[2000], best accuracy=78.26870421941916\n",
            "episode 2100:  domain loss: 0.6859, fsl loss: 0.0003, acc 0.9489, loss: 0.6861\n",
            "episode 2200:  domain loss: 0.7061, fsl loss: 0.0002, acc 0.9510, loss: 0.7063\n",
            "episode 2300:  domain loss: 0.7334, fsl loss: 0.0002, acc 0.9530, loss: 0.7336\n",
            "episode 2400:  domain loss: 0.6886, fsl loss: 0.0002, acc 0.9548, loss: 0.6888\n",
            "episode 2500:  domain loss: 0.6992, fsl loss: 0.0002, acc 0.9565, loss: 0.6994\n",
            "episode 2600:  domain loss: 0.7325, fsl loss: 0.2503, acc 0.9575, loss: 0.9829\n",
            "episode 2700:  domain loss: 0.6893, fsl loss: 0.0047, acc 0.9574, loss: 0.6940\n",
            "episode 2800:  domain loss: 0.6983, fsl loss: 0.0003, acc 0.9582, loss: 0.6986\n",
            "episode 2900:  domain loss: 0.7371, fsl loss: 0.0001, acc 0.9594, loss: 0.7372\n",
            "episode 3000:  domain loss: 0.6915, fsl loss: 0.0002, acc 0.9605, loss: 0.6917\n",
            "Testing ...\n",
            "6.742923736572266\n",
            "-5.025472640991211\n",
            "\t\tAccuracy: 31960/42731 (74.79%)\n",
            "\n",
            "best episode:[2000], best accuracy=78.26870421941916\n",
            "episode 3100:  domain loss: 0.6991, fsl loss: 0.0008, acc 0.9616, loss: 0.6998\n",
            "episode 3200:  domain loss: 0.7485, fsl loss: 0.0001, acc 0.9627, loss: 0.7486\n",
            "episode 3300:  domain loss: 0.6825, fsl loss: 0.0002, acc 0.9637, loss: 0.6827\n",
            "episode 3400:  domain loss: 0.6991, fsl loss: 0.0023, acc 0.9647, loss: 0.7014\n",
            "episode 3500:  domain loss: 0.7558, fsl loss: 0.0001, acc 0.9653, loss: 0.7559\n",
            "episode 3600:  domain loss: 0.6891, fsl loss: 0.0068, acc 0.9647, loss: 0.6959\n",
            "episode 3700:  domain loss: 0.6975, fsl loss: 0.0032, acc 0.9654, loss: 0.7007\n",
            "episode 3800:  domain loss: 0.7557, fsl loss: 0.0009, acc 0.9660, loss: 0.7566\n",
            "episode 3900:  domain loss: 0.6941, fsl loss: 0.0008, acc 0.9667, loss: 0.6949\n",
            "episode 4000:  domain loss: 0.6960, fsl loss: 0.0000, acc 0.9674, loss: 0.6961\n",
            "Testing ...\n",
            "8.204078674316406\n",
            "-6.698155879974365\n",
            "\t\tAccuracy: 32233/42731 (75.43%)\n",
            "\n",
            "best episode:[2000], best accuracy=78.26870421941916\n",
            "episode 4100:  domain loss: 0.7225, fsl loss: 0.0002, acc 0.9680, loss: 0.7227\n",
            "episode 4200:  domain loss: 0.7047, fsl loss: 0.0001, acc 0.9686, loss: 0.7047\n",
            "episode 4300:  domain loss: 0.7032, fsl loss: 0.0039, acc 0.9688, loss: 0.7071\n",
            "episode 4400:  domain loss: 0.7373, fsl loss: 0.0001, acc 0.9692, loss: 0.7374\n",
            "episode 4500:  domain loss: 0.6987, fsl loss: 0.0002, acc 0.9696, loss: 0.6989\n",
            "episode 4600:  domain loss: 0.6983, fsl loss: 0.0003, acc 0.9700, loss: 0.6986\n",
            "episode 4700:  domain loss: 0.7135, fsl loss: 0.0000, acc 0.9706, loss: 0.7135\n",
            "episode 4800:  domain loss: 0.7056, fsl loss: 0.0002, acc 0.9711, loss: 0.7058\n",
            "episode 4900:  domain loss: 0.6885, fsl loss: 0.0002, acc 0.9715, loss: 0.6887\n",
            "episode 5000:  domain loss: 0.7192, fsl loss: 0.0010, acc 0.9718, loss: 0.7201\n",
            "Testing ...\n",
            "8.320015907287598\n",
            "-5.200439929962158\n",
            "\t\tAccuracy: 31833/42731 (74.50%)\n",
            "\n",
            "best episode:[2000], best accuracy=78.26870421941916\n",
            "episode 5100:  domain loss: 0.6982, fsl loss: 0.0004, acc 0.9723, loss: 0.6986\n",
            "episode 5200:  domain loss: 0.6929, fsl loss: 0.0001, acc 0.9728, loss: 0.6930\n",
            "episode 5300:  domain loss: 0.7219, fsl loss: 0.0000, acc 0.9733, loss: 0.7219\n",
            "episode 5400:  domain loss: 0.6977, fsl loss: 0.0000, acc 0.9733, loss: 0.6977\n",
            "episode 5500:  domain loss: 0.6855, fsl loss: 0.0000, acc 0.9737, loss: 0.6855\n",
            "episode 5600:  domain loss: 0.7165, fsl loss: 0.0036, acc 0.9741, loss: 0.7201\n",
            "episode 5700:  domain loss: 0.7110, fsl loss: 0.0001, acc 0.9745, loss: 0.7111\n",
            "episode 5800:  domain loss: 0.6931, fsl loss: 0.0161, acc 0.9748, loss: 0.7092\n",
            "episode 5900:  domain loss: 0.7230, fsl loss: 0.0000, acc 0.9751, loss: 0.7231\n",
            "episode 6000:  domain loss: 0.7074, fsl loss: 0.0000, acc 0.9754, loss: 0.7075\n",
            "Testing ...\n",
            "7.806427478790283\n",
            "-5.247474670410156\n",
            "\t\tAccuracy: 33761/42731 (79.01%)\n",
            "\n",
            "save networks for episode: 6000\n",
            "/content/drive/MyDrive/DCFSL-2021-main/DCFSL-UP.py:630: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  A[iDataSet, :] = np.diag(C) / np.sum(C, 1, dtype=np.float)\n",
            "best episode:[6000], best accuracy=79.00821417706115\n",
            "episode 6100:  domain loss: 0.6987, fsl loss: 0.0000, acc 0.9758, loss: 0.6988\n",
            "episode 6200:  domain loss: 0.7175, fsl loss: 0.0001, acc 0.9761, loss: 0.7177\n",
            "episode 6300:  domain loss: 0.7026, fsl loss: 0.0005, acc 0.9765, loss: 0.7031\n",
            "episode 6400:  domain loss: 0.6938, fsl loss: 0.0000, acc 0.9769, loss: 0.6938\n",
            "episode 6500:  domain loss: 0.7069, fsl loss: 0.0000, acc 0.9772, loss: 0.7069\n",
            "episode 6600:  domain loss: 0.7078, fsl loss: 0.0059, acc 0.9775, loss: 0.7137\n",
            "episode 6700:  domain loss: 0.6995, fsl loss: 0.0016, acc 0.9777, loss: 0.7012\n",
            "episode 6800:  domain loss: 0.7181, fsl loss: 0.0003, acc 0.9780, loss: 0.7185\n",
            "episode 6900:  domain loss: 0.7058, fsl loss: 0.0000, acc 0.9783, loss: 0.7058\n",
            "episode 7000:  domain loss: 0.6964, fsl loss: 0.0000, acc 0.9785, loss: 0.6964\n",
            "Testing ...\n",
            "10.501558303833008\n",
            "-8.497117042541504\n",
            "\t\tAccuracy: 33088/42731 (77.43%)\n",
            "\n",
            "best episode:[6000], best accuracy=79.00821417706115\n",
            "episode 7100:  domain loss: 0.7208, fsl loss: 0.0008, acc 0.9787, loss: 0.7216\n",
            "episode 7200:  domain loss: 0.6905, fsl loss: 0.0128, acc 0.9785, loss: 0.7033\n",
            "episode 7300:  domain loss: 0.7066, fsl loss: 0.0005, acc 0.9786, loss: 0.7072\n",
            "episode 7400:  domain loss: 0.7562, fsl loss: 0.0001, acc 0.9788, loss: 0.7562\n",
            "episode 7500:  domain loss: 0.7200, fsl loss: 0.0001, acc 0.9791, loss: 0.7201\n",
            "episode 7600:  domain loss: 0.6998, fsl loss: 0.0000, acc 0.9793, loss: 0.6998\n",
            "episode 7700:  domain loss: 0.7064, fsl loss: 0.0000, acc 0.9796, loss: 0.7064\n",
            "episode 7800:  domain loss: 0.7051, fsl loss: 0.0001, acc 0.9798, loss: 0.7052\n",
            "episode 7900:  domain loss: 0.6948, fsl loss: 0.0000, acc 0.9800, loss: 0.6948\n",
            "episode 8000:  domain loss: 0.7021, fsl loss: 0.0000, acc 0.9803, loss: 0.7021\n",
            "Testing ...\n",
            "9.058981895446777\n",
            "-10.373085021972656\n",
            "\t\tAccuracy: 33996/42731 (79.56%)\n",
            "\n",
            "save networks for episode: 8000\n",
            "/content/drive/MyDrive/DCFSL-2021-main/DCFSL-UP.py:630: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  A[iDataSet, :] = np.diag(C) / np.sum(C, 1, dtype=np.float)\n",
            "best episode:[8000], best accuracy=79.55816620252276\n",
            "episode 8100:  domain loss: 0.6997, fsl loss: 0.0004, acc 0.9805, loss: 0.7001\n",
            "episode 8200:  domain loss: 0.6978, fsl loss: 0.0000, acc 0.9807, loss: 0.6979\n",
            "episode 8300:  domain loss: 0.7044, fsl loss: 0.0000, acc 0.9810, loss: 0.7044\n",
            "episode 8400:  domain loss: 0.7054, fsl loss: 0.0000, acc 0.9812, loss: 0.7054\n",
            "episode 8500:  domain loss: 0.6989, fsl loss: 0.0000, acc 0.9814, loss: 0.6990\n",
            "episode 8600:  domain loss: 0.7153, fsl loss: 0.0000, acc 0.9815, loss: 0.7153\n",
            "episode 8700:  domain loss: 0.7049, fsl loss: 0.0045, acc 0.9816, loss: 0.7093\n",
            "episode 8800:  domain loss: 0.6962, fsl loss: 0.0000, acc 0.9817, loss: 0.6962\n",
            "episode 8900:  domain loss: 0.7155, fsl loss: 0.0004, acc 0.9818, loss: 0.7160\n",
            "episode 9000:  domain loss: 0.6902, fsl loss: 0.0001, acc 0.9820, loss: 0.6903\n",
            "Testing ...\n",
            "10.141257286071777\n",
            "-9.082987785339355\n",
            "\t\tAccuracy: 32542/42731 (76.16%)\n",
            "\n",
            "best episode:[8000], best accuracy=79.55816620252276\n",
            "episode 9100:  domain loss: 0.7123, fsl loss: 0.0000, acc 0.9821, loss: 0.7123\n",
            "episode 9200:  domain loss: 0.7237, fsl loss: 0.0008, acc 0.9823, loss: 0.7245\n",
            "episode 9300:  domain loss: 0.7029, fsl loss: 0.0001, acc 0.9823, loss: 0.7031\n",
            "episode 9400:  domain loss: 0.6963, fsl loss: 0.0000, acc 0.9825, loss: 0.6963\n",
            "episode 9500:  domain loss: 0.7147, fsl loss: 0.0000, acc 0.9826, loss: 0.7147\n",
            "episode 9600:  domain loss: 0.7122, fsl loss: 0.0000, acc 0.9828, loss: 0.7122\n",
            "episode 9700:  domain loss: 0.7060, fsl loss: 0.0000, acc 0.9829, loss: 0.7060\n",
            "episode 9800:  domain loss: 0.7072, fsl loss: 0.0000, acc 0.9831, loss: 0.7072\n",
            "episode 9900:  domain loss: 0.7091, fsl loss: 0.0000, acc 0.9833, loss: 0.7091\n",
            "episode 10000:  domain loss: 0.7048, fsl loss: 0.0001, acc 0.9834, loss: 0.7049\n",
            "Testing ...\n",
            "10.20580768585205\n",
            "-13.53271484375\n",
            "\t\tAccuracy: 32464/42731 (75.97%)\n",
            "\n",
            "best episode:[8000], best accuracy=79.55816620252276\n",
            "iter:0 best episode:[8000], best accuracy=79.55816620252276\n",
            "***********************************************************************************\n",
            "train time per DataSet(s): 1871.34787\n",
            "test time per DataSet(s): 7.80109\n",
            "average OA: 79.56 +- 0.00\n",
            "average AA: 84.44 +- 0.00\n",
            "average kappa: 74.0016 +- 0.0000\n",
            "accuracy for each class: \n",
            "Class 0: 86.49 +- 0.00\n",
            "Class 1: 75.05 +- 0.00\n",
            "Class 2: 68.91 +- 0.00\n",
            "Class 3: 93.36 +- 0.00\n",
            "Class 4: 99.78 +- 0.00\n",
            "Class 5: 79.60 +- 0.00\n",
            "Class 6: 88.08 +- 0.00\n",
            "Class 7: 68.78 +- 0.00\n",
            "Class 8: 99.89 +- 0.00\n",
            "best accuracy obtained: \n",
            "99.89384288747345\n"
          ]
        }
      ]
    }
  ]
}